{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import abilities\n",
    "from ai import Chat, Vision, Audio, Images, Embeddings, FineTuner\n",
    "import powers\n",
    "\n",
    "# Adjust model globally here or modify \n",
    "# in function calls. Vision model cannot be changed.\n",
    "model = \"gpt-3.5-turbo-1106\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples\n",
    "\n",
    "Here are multiple examples of how to use the `ai.py` script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic completion call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = Chat(model=model, system=\"Respond like a frog.\")\n",
    "\n",
    "object = \"pizza\"\n",
    "\n",
    "# memories control whether the AI remembers the conversation or not, provided Chat is not reinitialized.\n",
    "completion, messages = chat.chat_completion(prompt=f\"Tell me a perfect haiku about {object}\", memories=True, seed=42, stream=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conversation with an LLM which retains memory of previous calls.\n",
    "\n",
    "- Use stream to see the response as it comes in.\n",
    "- Use speak to have the LLM speak the response out loud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = Audio()\n",
    "chat = Chat(model=model, system=\"Helpful assistant.\")\n",
    "\n",
    "while True:\n",
    "    transcript = audio.record_and_transcribe()\n",
    "    print(transcript)\n",
    "    completion, messages = chat.chat_completion(transcript, speak=True, stream=True, memories=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Moving memory from a conversation LLM to the vision LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = Chat(model=model, system=\"\")\n",
    "\n",
    "vision = Vision()\n",
    "\n",
    "response, messages = chat.chat_completion(prompt=\"What is your purpose?\", stream=False, memories=True)\n",
    "\n",
    "### This should respond with something like \"You just asked, 'What is your purpose?' if the memory transfer worked.\"\n",
    "response, messages = vision.vision_completion(prompt=\"What did I just say and how did you respond?\", messages=messages, stream = True, speak=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Infinite image generation and description using Vision and Images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = Images()\n",
    "\n",
    "response, path = images.generate_image(\"Black holes destroy London\", display_image=True, save_image=True)\n",
    "\n",
    "### issue with saving\n",
    "vision = Vision(system=\"You can only respond with bullet points.\")\n",
    "\n",
    "response, messages = vision.vision_completion(prompt=\"What is this image showing?\", image_paths=[path], memories=False, stream=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logit Bias to control the LLM's responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = Chat(model=model, system=\"\")\n",
    "\n",
    "# Applying a global bias of -100 to a list of phrases.\n",
    "phrases = [\"OpenAI\"]\n",
    "global_bias = -100\n",
    "\n",
    "## Return tokens and global bias to apply.\n",
    "logit_bias = chat.create_logit_bias(phrases, global_bias, augment=True)\n",
    "\n",
    "prompt = \"Only tell me the name of the company who developed you.\"\n",
    "\n",
    "response, messages = chat.chat_completion(prompt=prompt,\n",
    "                                          logit_bias=logit_bias, \n",
    "                                          stream=True, \n",
    "                                          memories=False,\n",
    "                                          seed=42)\n",
    "\n",
    "## Or, we can control the logit bias at a phrase level, by passing a dictionary of phrases and biases.\n",
    "\n",
    "phrases = {\"OpenAI\": -100, \"Google\": 18}\n",
    "\n",
    "## Return tokens and local bias to apply from phrases dict.\n",
    "logit_bias = chat.create_logit_bias(phrases, augment=True)\n",
    "\n",
    "##Â If you want to apply bias at token level, you need to construct the logit bias manually.\n",
    "\n",
    "response, messages = chat.chat_completion(prompt=prompt,\n",
    "                                          logit_bias=logit_bias, \n",
    "                                          stream=True, \n",
    "                                          memories=False,\n",
    "                                          seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logit bias Lobotomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ai import Chat\n",
    "\n",
    "# Initialize the chat model\n",
    "chat = Chat(model=model, system=\"Answer all questions.\")\n",
    "\n",
    "# Start with an empty set of banned words\n",
    "banned_words = set()\n",
    "\n",
    "# empty logit bias dictionary\n",
    "logit_bias = {}\n",
    "\n",
    "# Define a prompt for the AI\n",
    "prompt = \"What is 1+1?\"\n",
    "\n",
    "# Loop until the AI can no longer generate a response\n",
    "while True:\n",
    "    # Convert the banned words into token IDs and then into a logit bias dictionary with a high negative value to ban them\n",
    "    logit_bias = chat.create_logit_bias(list(banned_words), bias=-100, augment=False)\n",
    "\n",
    "    # Get the AI's response\n",
    "    response, messages = chat.chat_completion(prompt=prompt,\n",
    "                                              stream=False,\n",
    "                                              logit_bias=logit_bias,\n",
    "                                              memories=False,\n",
    "                                              seed=50)\n",
    "\n",
    "    # Check if the AI was able to generate a response\n",
    "    if not response.strip():\n",
    "        print(\"\\n------------------\\n\")\n",
    "        print(\"Lobotomy complete. AI failed to generate a response.\")\n",
    "        break\n",
    "\n",
    "    # Print the AI's response\n",
    "    print(response)\n",
    "\n",
    "    # Update the set of banned words with the words from the latest response\n",
    "    words_in_response = set(response.split())\n",
    "    banned_words.update(words_in_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embeddings for string similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the classes\n",
    "chat = Chat(model=\"gpt-4\", system = \"You create a prompt for an image to be generated about the given topic: \\n\\n\")\n",
    "image = Images()\n",
    "vision = Vision(system=\"Imagine you are reverse engineering the prompt used to generate this image. Give the prompt and nothing else.\")\n",
    "embedding = Embeddings()\n",
    "\n",
    "# Use the Chat class to generate a description of the scene\n",
    "print(\"Generating description...\")\n",
    "description, messages = chat.chat_completion(\"sausage person\")\n",
    "\n",
    "# Use the Image class to generate an image based on the description\n",
    "print(\"Generating image...\")\n",
    "image_response, image_path = image.generate_image(description, display_image=True, save_image=True)\n",
    "\n",
    "# Use the Vision class to describe the generated image\n",
    "print(\"Describing image...\")\n",
    "vision_response, messages = vision.vision_completion(\"\", image_paths=[image_path])\n",
    "\n",
    "# Calculate the cosine similarity between the original description and the image description\n",
    "print(\"Calculating similarity...\")\n",
    "similarity = embedding.string_similarity(description, vision_response)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Original description: {description}\")\n",
    "print(f\"Image description: {vision_response}\")\n",
    "print(f\"Cosine similarity: {similarity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieval Augmented Generation (RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = Embeddings()\n",
    "chat = Chat(model=model, system=\"Bullet points only. Answer the question using relevant information from the knowledge base.\")\n",
    "\n",
    "# Let's assume we have a knowledge base of facts\n",
    "knowledge_base = [\n",
    "\"Alice enjoys playing chess, walking and reading a good book. Sometimes she likes to go to the park and feed the ducks.\",\n",
    "\"Bob is a keen gardener. He likes to grow flowers and vegetables. He also likes to go for long walks in the countryside.\",\n",
    "\"Charlie is scared of the monsters his the bed, which he knows are real.\",\n",
    "\"Dennis really hates Charlie because he thinks he is an idiot and monsters don't exist.\",\n",
    "\"Eve is a very good cook. She likes to cook all kinds of food. Her favourite dish is spaghetti bolognese.\",\n",
    "\"Frank is a very successful businessman. He owns a chain of restaurants, a hotel and a casino.\",\n",
    "\"Grace is a very talented artist. She likes to paint portraits of her friends and family. She recently painted the monster under Charlie's bed.\",\n",
    "]\n",
    "# Example usage\n",
    "query = \"Describe the relationship between Dennis and Charlie. Could Frank help? Or is he the monster?\"\n",
    "\n",
    "retrieved_facts = powers.retrieval(embeddings, query, knowledge_base, top_n=10, similarity_threshold=0.75)\n",
    "\n",
    "facts_string = \", \".join(retrieved_facts)\n",
    "\n",
    "prompt = f\"{facts_string} \\n\\n: {query}\"\n",
    "\n",
    "response, messages = chat.chat_completion(prompt=prompt, memories=False, stream=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function calling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import powers\n",
    "\n",
    "chat = Chat(model=model, system=\"Helpful robot.\")\n",
    "\n",
    "# Dummy function to simulate an API call to get weather data\n",
    "def get_current_weather(location, unit=\"celsius\"):\n",
    "    # In a real scenario, this function would make an API call to a weather service\n",
    "    weather_data = {\n",
    "        \"Tokyo\": {\"temperature\": \"10\", \"unit\": unit},\n",
    "        \"San Francisco\": {\"temperature\": \"30\", \"unit\": unit},\n",
    "        \"Paris\": {\"temperature\": \"22\", \"unit\": unit}\n",
    "    }\n",
    "    return weather_data.get(location, {\"temperature\": \"unknown\"})\n",
    "\n",
    "# Dictionary mapping function names to actual function objects\n",
    "available_functions = {\n",
    "    \"get_current_weather\": get_current_weather,\n",
    "    # Add other functions here as needed\n",
    "}\n",
    "\n",
    "# Define the tool that the model can use\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_current_weather\",\n",
    "            \"description\": \"Get the current weather in a given location\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The city, e.g. San Francisco\",\n",
    "                    },\n",
    "                    \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n",
    "                },\n",
    "                \"required\": [\"location\"],\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "# Get the initial response from the model\n",
    "completion, messages, tool_calls = chat.chat_completion(\n",
    "    prompt=\"What's the temperature and weather in Paris vs Tokyo as a rhyming couplet?\",\n",
    "    tools=tools,\n",
    "    tool_choice=\"auto\",\n",
    "    return_tool_calls=True,\n",
    "    stream=False,\n",
    ")\n",
    "\n",
    "## Use tools is a power that applies any tools on the last message, and returns the updated messages with the responses from the tools integrated.   \n",
    "messages = powers.use_tools(messages, available_functions)\n",
    "\n",
    "# Get the final response from the model after the tool has been used\n",
    "completion, messages = chat.chat_completion(\n",
    "    prompt=\"\",\n",
    "    messages=messages,\n",
    "    stream=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text to speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = Audio()\n",
    "\n",
    "text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "audio.speak(text=text, voice='echo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finetune a model and then use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the FineTuner class\n",
    "fine_tuner = FineTuner()\n",
    "\n",
    "# Define the path to your JSONL file\n",
    "jsonl_file_path = 'finetune-data.jsonl'\n",
    "\n",
    "# Upload the file and start the fine-tuning process\n",
    "fine_tuning_job = fine_tuner.finetune_model(\n",
    "    file_path=jsonl_file_path,\n",
    "    batch_size='12',\n",
    "    learning_rate_multiplier='0.0001',\n",
    "    model_name='gpt-3.5-turbo',\n",
    "    suffix='example',\n",
    "    n_epochs=10,  # for example\n",
    ")\n",
    "\n",
    "# Print the fine-tuning job details\n",
    "print(fine_tuning_job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_job = fine_tuner.retrieve_finetuning_job(fine_tuning_job.id)\n",
    "\n",
    "finetuned_job.fine_tuned_model\n",
    "\n",
    "chat = Chat(model=finetuned_job.fine_tuned_model, system=\"\")\n",
    "\n",
    "response, messages = chat.chat_completion(prompt=\"What is the meaning of life?\", memories=False, stream=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Virtual Cooking Assistant with Image Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vision = Vision(system=\"Identify ingredients in the image and make a list.\")\n",
    "images = Images()\n",
    "\n",
    "# User uploads an image of ingredients they have\n",
    "ingredient_image_path = \"assets/fridge.jpg\"\n",
    "\n",
    "# Vision AI identifies the ingredients\n",
    "ingredients, messages = vision.vision_completion(prompt=\"Identify these ingredients.\", image_paths=[ingredient_image_path], stream=True)\n",
    "\n",
    "chat = Chat(model=\"gpt-3.5-turbo\", system=\"Generate a recipe based on the following ingredients.\")\n",
    "recipe_prompt = f\"Create a recipe using these ingredients: {ingredients}\"\n",
    "\n",
    "# Generate a recipe\n",
    "recipe, messages = chat.chat_completion(prompt=recipe_prompt, stream=True)\n",
    "\n",
    "# Generate an image of the dish\n",
    "response, dish_image_path = images.generate_image(prompt=recipe, display_image=True, save_image=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Language Translation Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = Chat(model=model) \n",
    "# Text to translate\\n\n",
    "original_text = 'Hello, how are you?'\n",
    "\n",
    "#Languages to translate into\\n\n",
    "# \n",
    "languages = ['Spanish', 'French', 'German', 'Japanese', 'Chinese', 'Russian', 'Arabic', 'Hindi', 'Portuguese', 'Italian']\n",
    "\n",
    "# Perform the translations\\n\n",
    "# \n",
    "for language in languages:\n",
    "    prompt = f'Translate \\\"{original_text}\\\" into {language}.'\n",
    "    response, messages = chat.chat_completion(prompt=prompt, stream=False)\n",
    "    print(f'{language}: {response}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitness Plan Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = Chat(model=model)\n",
    "\n",
    "# User's fitness goal\n",
    "fitness_goal = 'build muscle'\n",
    "\n",
    "# User's available days\n",
    "available_days = ['Monday', 'Wednesday', 'Friday']\n",
    "\n",
    "# Generate a workout plan\n",
    "response, messages = chat.chat_completion(prompt=f'Create a workout plan to {fitness_goal} for someone available on {available_days}.',\n",
    "                                        stream=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function calling for social media data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ai import Chat\n",
    "\n",
    "\n",
    "chat = Chat(model=model, system=\"Resourceful assistant.\")\n",
    "\n",
    "# Dummy function to simulate checking a user's social media mentions\n",
    "def check_social_mentions(username):\n",
    "    # In a real scenario, this function would interact with a social media API\n",
    "    mention_data = {\n",
    "        \"alice\": [\"Just had a great experience with @alice's bakery!\", \"@alice's new cake design looks amazing!\"],\n",
    "        \"bob\": [\"@bob's tech reviews are always so insightful.\", \"Can't wait for @bob's next podcast episode!\"],\n",
    "        \"carol\": [\"@carol's workout tips have really helped me improve my routine.\", \"So inspired by @carol's health journey!\"]\n",
    "    }\n",
    "    return mention_data.get(username, [\"No mentions found.\"])\n",
    "\n",
    "# Dictionary mapping function names to actual function objects\n",
    "available_functions = {\n",
    "    \"check_social_mentions\": check_social_mentions,\n",
    "    # Add other functions here as needed\n",
    "}\n",
    "\n",
    "# Define the tool that the model can use\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"check_social_mentions\",\n",
    "            \"description\": \"Check the latest social media mentions for a user\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"username\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The social media username to check\",\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"username\"],\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "# Get the initial response from the model\n",
    "completion, messages = chat.chat_completion(\n",
    "    prompt=\"What are people saying about bob on social media?\",\n",
    "    tools=tools,\n",
    "    tool_choice=\"auto\",\n",
    "    stream=False,\n",
    ")\n",
    "\n",
    "\n",
    "messages = powers.use_tools(messages, available_functions)\n",
    "\n",
    "# Get the final response from the model after the tool has been used\n",
    "completion, messages = chat.chat_completion(\n",
    "    prompt=\"reply as a sea shanty\",\n",
    "    messages=messages,\n",
    "    stream=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ainventory",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
